---
layout: post
category : Machine Learning
tagline:
tags : Classification
---
{% include JB/setup %}

##概述

下图是一个假想的邮件分类决策树.

![A decision tree in flowchart form](https://lh6.googleusercontent.com/-6AQJcHDG3Ps/UyJQG3oQ5TI/AAAAAAAAANs/IdP8KE3Yb5s/s800/decision-trees.png "A decision tree in flowchart form")

    优点: 低计算复杂度, 结果容易理解, 缺失值不敏感, 能够处理不相关特征数据;
    缺点: 易过度适应(Prone to overfitting);
    适用: 数值, 标称值.

##构造决策树

首先, 找到用于划分数据类别的特征项; 然后, 以此特征项分割不同类数据; 最后, 判断分割后的数据是否还能够分割, 如果可以, 再次重复这个过程.

伪码:
<script src="https://gist.github.com/jeffzhangfly/9540406.js"></script>

###流程

套用机器学习的几个步骤: 收集数据, 准备输入数据, 分析输入数据, 训练算法, 测试算法, 使用算法.

    1. 收集: 任意方法;
    2. 准备: 连续型数据需要首先离散化;
    3. 分析: 任意方法, 构造完成后visually inspect the tree;
    4. 训法: 构造一个决策树的数据结构;
    5. 测试: 计算错误率;
    6. 使用: ...

###信息增益

划分数据集要以将数据变得更有序/有组织(more organized)为标准. 使用信息论,检测数据分割前后的信息变化, 分割前后信息发生的变化即信息增益, 以信息增益最高为标准, 可以确定采用哪个特征来分隔数据最好.

###熵(entropy)

熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。

如果某个事物可能的分类有多个, 那么事物属于某个类别$x_i$的信息定义为

$$l(x_i)=-\log_2p(x_i)$$

$p(x_i)$是事物属于这个类别的概率.

熵定义为信息的期望值, 即所有位置的所有信息的期望值.

$$H=-\sum_{i=1}^np(x_i)\log_2p(x_i)$$

###算法

* [ID3](http://zh.wikipedia.org/zh/ID3算法 "ID3")
* [C4.5](http://zh.wikipedia.org/wiki/C4.5算法 "C4.5")

